{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuel\n",
    "\n",
    "## History\n",
    "\n",
    "* Started as a part of **Blocks**, a framework for building and managing **Theano** graphs in the context of neural networks.\n",
    "* Became its own project when we realized it was distinct enough that it could be used by other frameworks too.\n",
    "\n",
    "## Goal\n",
    "\n",
    "*Simplify downloading, storing, iterating over and preprocessing data used to train machine learning models.*\n",
    "\n",
    "## Quick start\n",
    "\n",
    "We'll go over a quick example to see how we can load arbitrary data into a dataset and and set up a basic preprocessing pipeline to iterate over the data while transforming it on the fly.\n",
    "\n",
    "Let's start by creating some random data to act as features and targets. We'll pretend that we have eight 2x2 grayscale images separated into four classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "seed = 1234\n",
    "rng = numpy.random.RandomState(seed)\n",
    "features = rng.randint(256, size=(8, 2, 2))\n",
    "targets = rng.randint(4, size=(8, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is to store this data in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IterableDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many dataset classes to choose from. We'll look at the simplest one, `IterableDataset`.\n",
    "\n",
    "It is created by passing a `dict` mapping source names to the data they contain and, optionally, a `dict` mapping source names to tuples of axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.datasets import IterableDataset\n",
    "dataset = IterableDataset(\n",
    "    iterables={'features': features, 'targets': targets},\n",
    "    axis_labels={'features': ('batch', 'height', 'width'),\n",
    "                 'targets': ('batch', 'index')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask the dataset what sources of data it provides by accessing its `sources` attribute. We can also know which axes correspond to what by accessing its `axis_labels` attribute. It also has a `num_examples` property telling us the number of examples it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Sources are {}.'.format(dataset.sources))\n",
    "print('Axis labels are {}.'.format(dataset.axis_labels))\n",
    "print('Dataset contains {} examples.'.format(dataset.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets themselves are stateless objects (as opposed to, say, an open file handle, or an iterator object). In order to request data from the dataset, we need to ask it to instantiate some stateful object with which it will interact. This is done through the `open` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state = dataset.open()\n",
    "print(state.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in `IterableDataset`'s case the state is an iterator object. We can now visit the examples this dataset contains using its `get_data` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dataset.get_data(state=state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the return order depends on the order of `dataset.sources`, which is nondeterministic if you use `dict` instances. In order to have deterministic behaviour, it is recommended that you use `OrderedDict` instances instead.\n",
    "\n",
    "Eventually, the iterator is depleted and it raises a `StopIteration` exception. We can iterate over the dataset again by requesting a fresh iterator through the dataset's `reset` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "while True:\n",
    "    try:\n",
    "        dataset.get_data(state=state)\n",
    "    except StopIteration:\n",
    "        print('Iteration over')\n",
    "        break\n",
    "state = dataset.reset(state=state)\n",
    "print(dataset.get_data(state=state))\n",
    "dataset.close(state=state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IndexableDataset\n",
    "\n",
    "The `IterableDataset` implementation is pretty minimal. For instance, it only lets you iterate sequentially and examplewise over your data.\n",
    "\n",
    "If your data happens to be indexable (e.g. a list, or a numpy array), then `IndexableDataset` will let you do much more.\n",
    "\n",
    "We instantiate `IndexableDataset` just like we would for `IterableDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.datasets import IndexableDataset\n",
    "from collections import OrderedDict\n",
    "\n",
    "dataset = IndexableDataset(\n",
    "    indexables=OrderedDict([('features', features), ('targets', targets)]),\n",
    "    axis_labels={'features': ('batch', 'height', 'width'), 'targets': ('batch', 'index')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of `IndexableDataset` over `IterableDataset` is that it allows random access of the data it contains. In order to do so, we need to pass an additional `request` argument to `get_data` in the form of a list of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state = dataset.open()\n",
    "print('State is {}'.format(state))\n",
    "print(dataset.get_data(state=state, request=[0, 1]))\n",
    "dataset.close(state=state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how `IndexableDataset` returns a `None` state: this is because there's no actual state to maintain in this case.\n",
    "\n",
    "### Iteration schemes\n",
    "\n",
    "Encapsulating and accessing our data is good, but if we're to integrate it into a training loop, we need to be able to iterate over the data. For that, we need to decide *which* indices to request and in *which order*. This is accomplished via an `IterationScheme` subclass.\n",
    "\n",
    "At its most basic level, an iteration scheme is responsible, through its `get_request_iterator` method, for building an iterator that will return requests. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.schemes import (SequentialScheme, ShuffledScheme,\n",
    "                          SequentialExampleScheme, ShuffledExampleScheme)\n",
    "\n",
    "schemes = [SequentialScheme(examples=8, batch_size=4),\n",
    "           ShuffledScheme(examples=8, batch_size=4),\n",
    "           SequentialExampleScheme(examples=8),\n",
    "           ShuffledExampleScheme(examples=8)]\n",
    "for scheme in schemes:\n",
    "    print([request for request in scheme.get_request_iterator()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can therefore use an iteration scheme to visit a dataset in some order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state = dataset.open()\n",
    "scheme = ShuffledScheme(examples=dataset.num_examples, batch_size=4)\n",
    "for request in scheme.get_request_iterator():\n",
    "    data = dataset.get_data(state=state, request=request)\n",
    "    print(data[0].shape, data[1].shape)\n",
    "dataset.close(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data streams\n",
    "\n",
    "Iteration schemes offer a more convenient way to visit the dataset than accessing the data by hand, but we can do better: the act of getting a fresh state from the dataset, getting a request iterator from the iteration scheme, using both to access the data and closing the state is repetitive. To automate this, we have data streams.\n",
    "\n",
    "The most common data stream class is `DataStream`. It is instantiated with a dataset and an iteration scheme, and returns an epoch iterator through its `get_epoch_iterator` method, which iterates over the dataset in the order defined by the iteration scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.streams import DataStream\n",
    "\n",
    "data_stream = DataStream(dataset=dataset, iteration_scheme=scheme)\n",
    "for data in data_stream.get_epoch_iterator():\n",
    "    print(data[0].shape, data[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "Some data streams take data streams as input. We call them *transformers*, and they enable us to build complex data preprocessing pipelines.\n",
    "\n",
    "Let's standardize the images we have by substracting their mean and dividing by their standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.transformers import ScaleAndShift\n",
    "# Note: ScaleAndShift applies (batch * scale) + shift, as\n",
    "# opposed to (batch + shift) * scale.\n",
    "scale = 1.0 / features.std()\n",
    "shift = - scale * features.mean()\n",
    "standardized_stream = ScaleAndShift(data_stream=data_stream,\n",
    "                                    scale=scale, shift=shift,\n",
    "                                    which_sources=('features',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting data stream can be used to iterate over the dataset just like before, but this time features will be standardized on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for batch in standardized_stream.get_epoch_iterator():\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's imagine that for some reason (e.g. running Theano code on GPU) we **need** features to have a data type of `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.transformers import Cast\n",
    "cast_standardized_stream = Cast(data_stream=standardized_stream,\n",
    "                                dtype='float32', which_sources=('features',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Fuel makes it easy to chain transformations to form a preprocessing pipeline. The complete pipeline now looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_stream = Cast(\n",
    "    ScaleAndShift(\n",
    "        DataStream(\n",
    "            dataset=dataset, iteration_scheme=scheme),   \n",
    "        scale=scale, shift=shift, which_sources=('features',)),\n",
    "    dtype='float32', which_sources=('features',))\n",
    "for batch in data_stream.get_epoch_iterator():\n",
    "    print(batch)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large datasets\n",
    "\n",
    "Sometimes, the dataset you're working on is too big to fit in memory. In that case, you'll want to use another common dataset class, `H5PYDataset`.\n",
    "\n",
    "### H5PYDataset\n",
    "\n",
    "As the name implies, `H5PYDataset` is a dataset class that interfaces with HDF5 files using the `h5py` library.\n",
    "\n",
    "HDF5 is a wonderful storage format, as it is organizable and self-documentable. This allows us to make a basic set of assumptions about the structure of an HDF5 file which, if met, greatly simplify creating new datasets and interacting with them. These assumptions are:\n",
    "\n",
    "* All data is stored into a single HDF5 file.\n",
    "* Data sources reside in the root group, and their names define the source names.\n",
    "* Data sources are not explicitly split into separate HDF5 datasets or separate HDF5 files. Instead, splits are defined in the `split` attribute of the root group.\n",
    "    \n",
    "Don't worry about that too much; Fuel has some built-in functions to take care of that.\n",
    "\n",
    "Let's create new random data. This time, we'll pretend that we're given a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_image_features = rng.randint(256, size=(90, 3, 32, 32)).astype('uint8')\n",
    "train_vector_features = rng.normal(size=(90, 16))\n",
    "train_targets = rng.randint(10, size=(90, 1)).astype('uint8')\n",
    "\n",
    "test_image_features = rng.randint(256, size=(10, 3, 32, 32)).astype('uint8')\n",
    "test_vector_features = rng.normal(size=(10, 16))\n",
    "test_targets = rng.randint(10, size=(10, 1)).astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would normally need to\n",
    "* open an HDF5 file for writing,\n",
    "* create three datasets in the root group, one for each data source,\n",
    "* fill the datasets with our training and test data and\n",
    "* build the split array describing how the file is laid out.\n",
    "\n",
    "Fortunately for us, Fuel has a built-in function that automates the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from fuel.converters.base import fill_hdf5_file\n",
    "f = h5py.File('dataset.hdf5', mode='w')\n",
    "data = (('train', 'image_features', train_image_features),\n",
    "        ('train', 'vector_features', train_vector_features),\n",
    "        ('train', 'targets', train_targets),\n",
    "        ('test', 'image_features', test_image_features),\n",
    "        ('test', 'vector_features', test_vector_features),\n",
    "        ('test', 'targets', test_targets))\n",
    "fill_hdf5_file(f, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before closing the file, let's also tag axes with their label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, label in enumerate(('batch', 'channel', 'height', 'width')):\n",
    "    f['image_features'].dims[i].label = label\n",
    "for i, label in enumerate(('batch', 'feature')):\n",
    "    f['vector_features'].dims[i].label = label\n",
    "for i, label in enumerate(('batch', 'index')):\n",
    "    f['targets'].dims[i].label = label\n",
    "f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything we need to load this HDF5 file in Fuel.\n",
    "\n",
    "We'll instantiate `H5PYDataset` by passing it the path to our HDF5 file as well as a tuple of splits to use. For now, we'll just load the train and test sets separately, but note that it is also possible to concatenate splits that way (e.g. concatenate the training and validation sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.datasets import H5PYDataset\n",
    "train_dataset = H5PYDataset('dataset.hdf5', which_sets=('train',))\n",
    "test_dataset = H5PYDataset('dataset.hdf5', which_sets=('test',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`H5PYDataset` instances allow the same level of introspection as `IndexableDataset` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Sources are {}.'.format(train_dataset.sources))\n",
    "print('Axis labels are {}.'.format(train_dataset.axis_labels))\n",
    "print('Training set contains {} examples.'.format(train_dataset.num_examples))\n",
    "print('Test set contains {} examples.'.format(test_dataset.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate over data the same way as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_stream = DataStream(\n",
    "    dataset=train_dataset,\n",
    "    iteration_scheme=ShuffledScheme(\n",
    "        examples=train_dataset.num_examples, batch_size=10))\n",
    "for batch in train_stream.get_epoch_iterator():\n",
    "    print([source.shape for source in batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H5PYDataset for small datasets\n",
    "\n",
    "The `H5PYDataset` class isn't suitable only to large datasets. In fact, most of Fuel's built-in datasets rely on  HDF5 for storage.\n",
    "\n",
    "At first sight, this might seem inefficient, but `H5PYDataset` features a `load_in_memory` constructor argument which, when set to `True`, reads data off disk once and stores it in memory as a numpy array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in datasets\n",
    "\n",
    "### Defining where Fuel looks for data\n",
    "\n",
    "You can tell Fuel where to look for data by setting the `data_path` variable in `~/.fuelrc`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ~/.fuelrc\n",
    "\n",
    "data_path: /home/user/data_location_1:/home/user/data_location_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can override it by setting the `FUEL_DATA_PATH` environment variable.\n",
    "\n",
    "In both cases, Fuel expects a sequence of paths separated by an OS-specific delimiter (`:` for Linux / Mac OS, `;` for Windows).\n",
    "\n",
    "Let's create a directory in which to put our data files and set it as our Fuel data path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir fuel_data\n",
    "import os\n",
    "os.environ['FUEL_DATA_PATH'] = os.path.abspath('./fuel_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading raw data files\n",
    "\n",
    "Fuel comes with a `fuel-download` script which automates downloading raw data files for built-in datasets. You can have a look at the full list of built-in datasets with `fuel-download -h`. For now, we'll download the MNIST dataset in our newly-created data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!fuel-download mnist -d $FUEL_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting raw data files\n",
    "\n",
    "We'll now convert the raw data files to an HDF5 file which the `MNIST` dataset class can read. This is done using the `fuel-convert` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!fuel-convert mnist -d $FUEL_DATA_PATH -o $FUEL_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using built-in datasets\n",
    "\n",
    "Now that the data has been downloaded and converted, we can instantiate and use the built-in dataset class just like any other `H5PYDataset` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets import MNIST\n",
    "from matplotlib import pyplot, cm\n",
    "\n",
    "dataset = MNIST(('train',), sources=('features',))\n",
    "state = dataset.open()\n",
    "image, = dataset.get_data(state=state, request=[1234])\n",
    "pyplot.imshow(image.reshape((28, 28)), cmap=cm.Greys_r, interpolation='nearest')\n",
    "pyplot.show()\n",
    "dataset.close(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Fuel\n",
    "\n",
    "### New dataset classes\n",
    "\n",
    "### New transformers\n",
    "\n",
    "### New iteration schemes\n",
    "\n",
    "## Parallelizing data processing\n",
    "\n",
    "## Common tasks\n",
    "\n",
    "### Preprocess once"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
